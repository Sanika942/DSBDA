{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ac1cbf-cf91-4b0e-bd36-6b40a6c4cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3695540d-7dcf-4062-a313-cd1b47b188ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I will walk 500 miles and I would walk 500 more. Just to be the man who walks \" + \\\n",
    "            \"a thousand miles to fall down at your door!\"\n",
    "sentence2 = \"I played the play playfully as the players were playing in the play with playfullness\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b272426-c556-4b65-8cb3-3292cae0e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1: I will walk 500 miles and I would walk 500 more. Just to be the man who walks a thousand miles to fall down at your door!\n",
      "sentence2: I played the play playfully as the players were playing in the play with playfullness\n"
     ]
    }
   ],
   "source": [
    "print(\"sentence1:\",sentence1)\n",
    "print(\"sentence2:\",sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb9e2c48-3950-4d16-bf6d-d372b519e84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0fd7e6e-549e-4466-ba45-15af17573947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['I', 'will', 'walk', '500', 'miles', 'and', 'I', 'would', 'walk', '500', 'more', '.', 'Just', 'to', 'be', 'the', 'man', 'who', 'walks', 'a', 'thousand', 'miles', 'to', 'fall', 'down', 'at', 'your', 'door', '!']\n",
      "\n",
      "Tokenized sentences: ['I will walk 500 miles and I would walk 500 more.', 'Just to be the man who walks a thousand miles to fall down at your door!']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "print('Tokenized words:', word_tokenize(sentence1))\n",
    "print('\\nTokenized sentences:', sent_tokenize(sentence1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa63f089-7ec8-4156-843c-57a7c6d711d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1155fcb6-9f20-4da8-8262-99c90336a2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging Parts of Speech: [('I', 'PRP'), ('will', 'MD'), ('walk', 'VB'), ('500', 'CD'), ('miles', 'NNS'), ('and', 'CC'), ('I', 'PRP'), ('would', 'MD'), ('walk', 'VB'), ('500', 'CD'), ('more', 'JJR'), ('.', '.'), ('Just', 'NNP'), ('to', 'TO'), ('be', 'VB'), ('the', 'DT'), ('man', 'NN'), ('who', 'WP'), ('walks', 'VBZ'), ('a', 'DT'), ('thousand', 'NN'), ('miles', 'NNS'), ('to', 'TO'), ('fall', 'VB'), ('down', 'RP'), ('at', 'IN'), ('your', 'PRP$'), ('door', 'NN'), ('!', '.'), ('I', 'PRP'), ('played', 'VBD'), ('the', 'DT'), ('play', 'NN'), ('playfully', 'RB'), ('as', 'IN'), ('the', 'DT'), ('players', 'NNS'), ('were', 'VBD'), ('playing', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('play', 'NN'), ('with', 'IN'), ('playfullness', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "token = word_tokenize(sentence1) + word_tokenize(sentence2)\n",
    "tagged = pos_tag(token)                 \n",
    "\n",
    "print(\"Tagging Parts of Speech:\", tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cab00cd7-e638-4225-8072-22f226116f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48cdd2fa-4b79-4c1f-9ffc-5b1733227651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclean version: ['I', 'will', 'walk', '500', 'miles', 'and', 'I', 'would', 'walk', '500', 'more', '.', 'Just', 'to', 'be', 'the', 'man', 'who', 'walks', 'a', 'thousand', 'miles', 'to', 'fall', 'down', 'at', 'your', 'door', '!']\n",
      "\n",
      "Cleaned version: ['I', 'walk', '500', 'miles', 'I', 'would', 'walk', '500', '.', 'Just', 'man', 'walks', 'thousand', 'miles', 'fall', 'door', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "token = word_tokenize(sentence1)\n",
    "cleaned_token = []\n",
    "\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "\n",
    "print('Unclean version:', token)\n",
    "print('\\nCleaned version:', cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "007bd895-545f-4eec-950a-3bb2108b5de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words: ['i', 'play', 'the', 'play', 'play', 'as', 'the', 'player', 'were', 'play', 'in', 'the', 'play', 'with', 'playful']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "token = word_tokenize(sentence2)\n",
    "\n",
    "stemmed_words = []\n",
    "for word in token:\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    stemmed_words.append(stemmed_word)\n",
    "\n",
    "# Print the stemmed words\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8540b4c-c9ab-4faa-9b34-f5a7fc5fe8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sai\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48a63115-27e5-44d5-920a-d7ea2825e4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I played the play playfully as the players were playing in the play with playfullness\n",
      "Lemmatized words: ['I', 'played', 'the', 'play', 'playfully', 'a', 'the', 'player', 'were', 'playing', 'in', 'the', 'play', 'with', 'playfullness']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "token = word_tokenize(sentence2)\n",
    "lemmatized_output = []\n",
    "\n",
    "for word in tokenized_words:\n",
    "    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "    lemmatized_output.append(lemmatized_word)\n",
    "\n",
    "print(\"Lemmatized words:\", lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "337cc336-b3db-4420-ae07-14aa2dda5698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency (TF):\n",
      "500:\t  0.08333333333333333\n",
      "and:\t  0.041666666666666664\n",
      "at:\t  0.041666666666666664\n",
      "be:\t  0.041666666666666664\n",
      "door:\t  0.041666666666666664\n",
      "down:\t  0.041666666666666664\n",
      "fall:\t  0.041666666666666664\n",
      "just:\t  0.041666666666666664\n",
      "man:\t  0.041666666666666664\n",
      "miles:\t  0.08333333333333333\n",
      "more:\t  0.041666666666666664\n",
      "the:\t  0.041666666666666664\n",
      "thousand:\t  0.041666666666666664\n",
      "to:\t  0.08333333333333333\n",
      "walk:\t  0.08333333333333333\n",
      "walks:\t  0.041666666666666664\n",
      "who:\t  0.041666666666666664\n",
      "will:\t  0.041666666666666664\n",
      "would:\t  0.041666666666666664\n",
      "your:\t  0.041666666666666664\n",
      "\n",
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "500:\t 1.0\n",
      "and:\t 1.0\n",
      "at:\t 1.0\n",
      "be:\t 1.0\n",
      "door:\t 1.0\n",
      "down:\t 1.0\n",
      "fall:\t 1.0\n",
      "just:\t 1.0\n",
      "man:\t 1.0\n",
      "miles:\t 1.0\n",
      "more:\t 1.0\n",
      "the:\t 1.0\n",
      "thousand:\t 1.0\n",
      "to:\t 1.0\n",
      "walk:\t 1.0\n",
      "walks:\t 1.0\n",
      "who:\t 1.0\n",
      "will:\t 1.0\n",
      "would:\t 1.0\n",
      "your:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([text])\n",
    "\n",
    "# Get the feature names (terms)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the IDF values\n",
    "idf_values = vectorizer.idf_\n",
    "\n",
    "# Calculate the TF values manually\n",
    "tf_matrix = tfidf_matrix.toarray()\n",
    "tf_values = tf_matrix / np.sum(tf_matrix, axis=1).reshape(-1, 1)\n",
    "\n",
    "# Create a dictionary with terms and their TF and IDF values\n",
    "tf_dict = dict(zip(terms, tf_values[0]))\n",
    "idf_dict = dict(zip(terms, idf_values))\n",
    "\n",
    "# Print the TF values for each term\n",
    "print(\"Term Frequency (TF):\")\n",
    "for term, value in tf_dict.items():\n",
    "    print(f\"{term}:\\t  {value}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print the IDF values for each term\n",
    "print(\"Inverse Document Frequency (IDF):\")\n",
    "for term, value in idf_dict.items():\n",
    "    print(f\"{term}:\\t {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b98798-60dc-479d-8fd7-29ddc43a37f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500: 0.35355339059327373\n",
      "and: 0.17677669529663687\n",
      "at: 0.17677669529663687\n",
      "be: 0.17677669529663687\n",
      "door: 0.17677669529663687\n",
      "down: 0.17677669529663687\n",
      "fall: 0.17677669529663687\n",
      "just: 0.17677669529663687\n",
      "man: 0.17677669529663687\n",
      "miles: 0.35355339059327373\n",
      "more: 0.17677669529663687\n",
      "the: 0.17677669529663687\n",
      "thousand: 0.17677669529663687\n",
      "to: 0.35355339059327373\n",
      "walk: 0.35355339059327373\n",
      "walks: 0.17677669529663687\n",
      "who: 0.17677669529663687\n",
      "will: 0.17677669529663687\n",
      "would: 0.17677669529663687\n",
      "your: 0.17677669529663687\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = vectorizer.fit_transform([text])\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "# Get the TF-IDF values\n",
    "tfidf_values = tfidf_matrix.toarray()[0]\n",
    "\n",
    "# Create a dictionary with terms and their TF-IDF values\n",
    "tfidf_dict = dict(zip(terms, tfidf_values))\n",
    "# Print the TF-IDF values for each term\n",
    "for term, value in tfidf_dict.items():\n",
    "    print(f\"{term}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddc398-f224-4960-b5d4-baaece6ff81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
